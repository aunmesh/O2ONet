{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotation file\n",
    "# Consider a particular annotation\n",
    "# Load the corresponding gif\n",
    "# Track the bounding boxes\n",
    "# Repurpose the IKEA-ASM feature extraction code to extract the features\n",
    "# Will need to implement the code for I3D network or repurpose the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotation file\n",
    "anno_path = '/workspace/work/O2ONet/data/annotations_minus_unavailable_yt_vids.pkl'\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "f = open(anno_path, 'rb')\n",
    "anno = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracker\n",
    "def tracker_old(frames, main_bbox_tb):\n",
    "    import cv2\n",
    "    import sys\n",
    "    \n",
    "    image_height, image_width,_ = frames[0].shape\n",
    "\n",
    "    main_bbox_wh = (\n",
    "                    main_bbox_tb[0], \n",
    "                    main_bbox_tb[1],\n",
    "                    main_bbox_tb[2]-main_bbox_tb[0],\n",
    "                    main_bbox_tb[3]-main_bbox_tb[1]\n",
    "                    )\n",
    "\n",
    "\n",
    "    # main_bbox_wh = (\n",
    "    #                 max(main_bbox_tb[0], 1), \n",
    "    #                 max(main_bbox_tb[1], 1),\n",
    "    #                 max(main_bbox_tb[2]-main_bbox_tb[0], 1),\n",
    "    #                 max(main_bbox_tb[3]-main_bbox_tb[1], 1)\n",
    "    #                 )\n",
    "    \n",
    "    (major_ver, minor_ver, subminor_ver) = cv2.__version__.split('.')\n",
    "\n",
    "\n",
    "    # Set up tracker.\n",
    "    # Instead of MIL, you can also use\n",
    "\n",
    "    tracker_types = ['BOOSTING', 'MIL','KCF', 'TLD', 'MEDIANFLOW', 'GOTURN', 'MOSSE', 'CSRT']\n",
    "    tracker_type = tracker_types[-2]\n",
    "\n",
    "    if int(minor_ver) < 3:\n",
    "        tracker = cv2.Tracker_create(tracker_type)\n",
    "    else:\n",
    "        if tracker_type == 'BOOSTING':\n",
    "            tracker = cv2.TrackerBoosting_create()\n",
    "            tracker_rev = cv2.TrackerBoosting_create()\n",
    "        if tracker_type == 'MIL':\n",
    "            tracker = cv2.TrackerMIL_create()\n",
    "            tracker_rev = cv2.TrackerMIL_create()\n",
    "        if tracker_type == 'KCF':\n",
    "            tracker = cv2.TrackerKCF_create()\n",
    "            tracker_rev = cv2.TrackerKCF_create()\n",
    "        if tracker_type == 'TLD':\n",
    "            tracker = cv2.legacy_TrackerTLD.create()\n",
    "            tracker_rev = cv2.legacy_TrackerTLD.create()\n",
    "        if tracker_type == 'MEDIANFLOW':\n",
    "            tracker = cv2.legacy_TrackerMedianFlow.create()\n",
    "            tracker_rev = cv2.legacy_TrackerMedianFlow.create()\n",
    "        if tracker_type == 'GOTURN':\n",
    "            tracker = cv2.TrackerGOTURN_create()\n",
    "            tracker_rev = cv2.TrackerGOTURN_create()\n",
    "        if tracker_type == 'MOSSE':\n",
    "            tracker = cv2.legacy_TrackerMOSSE.create()\n",
    "            tracker_rev = cv2.legacy_TrackerMOSSE.create()\n",
    "        if tracker_type == \"CSRT\":\n",
    "            tracker = cv2.TrackerCSRT_create()\n",
    "            tracker_rev = cv2.TrackerCSRT_create()\n",
    "\n",
    "    num_frames = len(frames)\n",
    "\n",
    "    central_index = int((num_frames - 1)/2)\n",
    "    window_size = int(num_frames/2)\n",
    "\n",
    "    central_frame = frames[central_index]\n",
    "\n",
    "    # Initialize tracker with first frame and bounding box\n",
    "    \n",
    "    ok = tracker.init(central_frame, main_bbox_wh)\n",
    "    bboxes_forward = []\n",
    "\n",
    "    for i in range(window_size):\n",
    "\n",
    "        # Read a new frame\n",
    "        frame = frames[central_index + 1 + i]        \n",
    "\n",
    "        # Update tracker\n",
    "        ok, bbox_wh = tracker.update(frame)\n",
    "\n",
    "        # add to the bbox list\n",
    "        if ok:\n",
    "            bbox_tb = [ bbox_wh[0], bbox_wh[1], bbox_wh[0] + bbox_wh[2], bbox_wh[1] + bbox_wh[3] ]\n",
    "            import numpy as np\n",
    "\n",
    "            bbox_tb[0], bbox_tb[2] = np.clip(bbox_tb[0],0, image_width-1), np.clip(bbox_tb[2],0, image_width-1)\n",
    "            bbox_tb[1], bbox_tb[3] = np.clip(bbox_tb[1],0, image_height-1), np.clip(bbox_tb[3],0, image_height-1)\n",
    "\n",
    "            bboxes_forward.append(bbox_tb)\n",
    "            # # Tracking success\n",
    "            # p1 = (int(bbox[0]), int(bbox[1]))\n",
    "            # p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "            # cv2.rectangle(frame, p1, p2, (255,0,0), 2, 1)\n",
    "        else :\n",
    "            print(\"Tracking Failure\")\n",
    "            return 0\n",
    "            # Tracking failure\n",
    "            # cv2.putText(frame, \"Tracking failure detected\", (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2)\n",
    "\n",
    "    # Initialize tracker with first frame and bounding box\n",
    "    ok = tracker_rev.init(central_frame, main_bbox_wh)\n",
    "    bboxes_backward = []\n",
    "    for i in range(window_size):\n",
    "        \n",
    "        # Read a new frame\n",
    "        frame = frames[central_index - 1 - i]        \n",
    "        \n",
    "        # Update tracker\n",
    "        ok, bbox_wh = tracker_rev.update(frame)\n",
    "\n",
    "        # Add to the bbox list\n",
    "        if ok:\n",
    "            bbox_tb = [ bbox_wh[0], bbox_wh[1], bbox_wh[0] + bbox_wh[2], bbox_wh[1] + bbox_wh[3] ]\n",
    "\n",
    "            bbox_tb[0], bbox_tb[2] = np.clip(bbox_tb[0],0, image_width-1), np.clip(bbox_tb[2],0, image_width-1)\n",
    "            bbox_tb[1], bbox_tb[3] = np.clip(bbox_tb[1],0, image_height-1), np.clip(bbox_tb[3],0, image_height-1)\n",
    "\n",
    "            bboxes_backward.append(bbox_tb)\n",
    "            # # Tracking success\n",
    "            # p1 = (int(bbox[0]), int(bbox[1]))\n",
    "            # p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "            # cv2.rectangle(frame, p1, p2, (255,0,0), 2, 1)\n",
    "        else:\n",
    "            print(\"Tracking Failure\")\n",
    "            return 0\n",
    "            # Tracking failure\n",
    "            # cv2.putText(frame, \"Tracking failure detected\", (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2)\n",
    "\n",
    "    del tracker\n",
    "    del tracker_rev\n",
    "    bboxes_backward_reversed = bboxes_backward[-1::-1]\n",
    "    all_bbox = bboxes_backward_reversed + [main_bbox_tb] + bboxes_forward\n",
    "    \n",
    "    return all_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracker with recovery\n",
    "def tracker(frames, main_bbox_tb):\n",
    "    import cv2\n",
    "    import sys\n",
    "    \n",
    "    image_height, image_width,_ = frames[0].shape\n",
    "\n",
    "    main_bbox_wh = (\n",
    "                    main_bbox_tb[0], \n",
    "                    main_bbox_tb[1],\n",
    "                    main_bbox_tb[2]-main_bbox_tb[0],\n",
    "                    main_bbox_tb[3]-main_bbox_tb[1]\n",
    "                    )\n",
    "    (major_ver, minor_ver, subminor_ver) = cv2.__version__.split('.')\n",
    "\n",
    "\n",
    "    # Set up tracker.\n",
    "    # Instead of MIL, you can also use\n",
    "\n",
    "    tracker_types = ['BOOSTING', 'MIL','KCF', 'TLD', 'MEDIANFLOW', 'GOTURN', 'MOSSE', 'CSRT']\n",
    "    tracker_type = tracker_types[-1]\n",
    "\n",
    "    if int(minor_ver) < 3:\n",
    "        tracker = cv2.Tracker_create(tracker_type)\n",
    "    else:\n",
    "        if tracker_type == 'BOOSTING':\n",
    "            tracker = cv2.TrackerBoosting_create()\n",
    "            tracker_rev = cv2.TrackerBoosting_create()\n",
    "        if tracker_type == 'MIL':\n",
    "            tracker = cv2.TrackerMIL_create()\n",
    "            tracker_rev = cv2.TrackerMIL_create()\n",
    "        if tracker_type == 'KCF':\n",
    "            tracker = cv2.TrackerKCF_create()\n",
    "            tracker_rev = cv2.TrackerKCF_create()\n",
    "        if tracker_type == 'TLD':\n",
    "            tracker = cv2.legacy_TrackerTLD.create()\n",
    "            tracker_rev = cv2.legacy_TrackerTLD.create()\n",
    "        if tracker_type == 'MEDIANFLOW':\n",
    "            tracker = cv2.legacy_TrackerMedianFlow.create()\n",
    "            tracker_rev = cv2.legacy_TrackerMedianFlow.create()\n",
    "        if tracker_type == 'GOTURN':\n",
    "            tracker = cv2.TrackerGOTURN_create()\n",
    "            tracker_rev = cv2.TrackerGOTURN_create()\n",
    "        if tracker_type == 'MOSSE':\n",
    "            tracker = cv2.legacy_TrackerMOSSE.create()\n",
    "            tracker_rev = cv2.legacy_TrackerMOSSE.create()\n",
    "        if tracker_type == \"CSRT\":\n",
    "            tracker = cv2.TrackerCSRT_create()\n",
    "            tracker_rev = cv2.TrackerCSRT_create()\n",
    "\n",
    "    num_frames = len(frames)\n",
    "\n",
    "    central_index = int((num_frames - 1)/2)\n",
    "    window_size = int(num_frames/2)\n",
    "\n",
    "    central_frame = frames[central_index]\n",
    "\n",
    "    # Initialize tracker with first frame and bounding box\n",
    "    \n",
    "    ok = tracker.init(central_frame, main_bbox_wh)\n",
    "    bboxes_forward = []\n",
    "\n",
    "    for i in range(window_size):\n",
    "\n",
    "        # Read a new frame\n",
    "        frame = frames[central_index + 1 + i]        \n",
    "\n",
    "        # Update tracker\n",
    "        ok, bbox_wh = tracker.update(frame)\n",
    "        if not ok:\n",
    "            print(bbox_wh)\n",
    "        # add to the bbox list\n",
    "        if ok:\n",
    "            bbox_tb = [ bbox_wh[0], bbox_wh[1], bbox_wh[0] + bbox_wh[2], bbox_wh[1] + bbox_wh[3] ]\n",
    "            import numpy as np\n",
    "\n",
    "            bbox_tb[0], bbox_tb[2] = np.clip(bbox_tb[0],0, image_width-1), np.clip(bbox_tb[2],0, image_width-1)\n",
    "            bbox_tb[1], bbox_tb[3] = np.clip(bbox_tb[1],0, image_height-1), np.clip(bbox_tb[3],0, image_height-1)\n",
    "\n",
    "            bboxes_forward.append(bbox_tb)\n",
    "            # # Tracking success\n",
    "            # p1 = (int(bbox[0]), int(bbox[1]))\n",
    "            # p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "            # cv2.rectangle(frame, p1, p2, (255,0,0), 2, 1)\n",
    "        else :\n",
    "            print(\"Tracking Failure\")\n",
    "            if len(bboxes_forward) > 0:\n",
    "                bboxes_forward.append(bboxes_forward[-1])\n",
    "            else:\n",
    "                bboxes_forward.append(main_bbox_tb)\n",
    "            # bbox_wh = bboxes\n",
    "            # return 0\n",
    "            # Tracking failure\n",
    "            # cv2.putText(frame, \"Tracking failure detected\", (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2)\n",
    "\n",
    "    # Initialize tracker with first frame and bounding box\n",
    "    ok = tracker_rev.init(central_frame, main_bbox_wh)\n",
    "    bboxes_backward = []\n",
    "    for i in range(window_size):\n",
    "        \n",
    "        # Read a new frame\n",
    "        frame = frames[central_index - 1 - i]        \n",
    "        \n",
    "        # Update tracker\n",
    "        ok, bbox_wh = tracker_rev.update(frame)\n",
    "\n",
    "        # Add to the bbox list\n",
    "        if ok:\n",
    "            bbox_tb = [ bbox_wh[0], bbox_wh[1], bbox_wh[0] + bbox_wh[2], bbox_wh[1] + bbox_wh[3] ]\n",
    "\n",
    "            bbox_tb[0], bbox_tb[2] = np.clip(bbox_tb[0],0, image_width-1), np.clip(bbox_tb[2],0, image_width-1)\n",
    "            bbox_tb[1], bbox_tb[3] = np.clip(bbox_tb[1],0, image_height-1), np.clip(bbox_tb[3],0, image_height-1)\n",
    "\n",
    "            bboxes_backward.append(bbox_tb)\n",
    "            # # Tracking success\n",
    "            # p1 = (int(bbox[0]), int(bbox[1]))\n",
    "            # p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "            # cv2.rectangle(frame, p1, p2, (255,0,0), 2, 1)\n",
    "        else:\n",
    "            print(\"Tracking Failure\")\n",
    "            if len(bboxes_backward) > 0:\n",
    "                bboxes_backward.append(bboxes_backward[-1])\n",
    "            else:\n",
    "                bboxes_backward.append(main_bbox_tb)\n",
    "            # return 0\n",
    "            # Tracking failure\n",
    "            # cv2.putText(frame, \"Tracking failure detected\", (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2)\n",
    "\n",
    "    del tracker\n",
    "    del tracker_rev\n",
    "    bboxes_backward_reversed = bboxes_backward[-1::-1]\n",
    "    all_bbox = bboxes_backward_reversed + [main_bbox_tb] + bboxes_forward\n",
    "    \n",
    "    return all_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_tracking(frames, bboxes):\n",
    "\n",
    "        import cv2\n",
    "        vis_frames = []\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "\n",
    "            bbox = bboxes[i]\n",
    "            p1 = ( int(bbox[0]), int(bbox[1]) )\n",
    "            p2 = ( int(bbox[2]), int(bbox[3]) )\n",
    "            temp_frame = cv2.rectangle(frame, p1, p2, (255,0,0), 2, 1)\n",
    "            rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            vis_frames.append(rgb_frame)\n",
    "        \n",
    "        import imageio\n",
    "        fps = 4\n",
    "        imageio.mimsave( './visualisation.gif', vis_frames, fps=4)\n",
    "\n",
    "def track_bbox(anno, gif_folder):\n",
    "\n",
    "    bbox = anno['bboxes']['3']['bbox']\n",
    "    # bbox = [1030, 0, 1277, 720]\n",
    "    yt_id = anno['metadata']['yt_id']\n",
    "    frame_index = anno['metadata']['frame no.']\n",
    "    window_size = 5\n",
    "    \n",
    "    filename = yt_id + '_' + str(frame_index) + '_' + str(window_size) + '.gif'\n",
    "    import os\n",
    "    file_location = os.path.join(gif_folder, filename)\n",
    "    import cv2\n",
    "    vid = cv2.VideoCapture(file_location)\n",
    "    frames = []\n",
    "\n",
    "    frame_count = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    for i in range(frame_count):\n",
    "        success, frame = vid.read()\n",
    "        frames.append(frame)\n",
    "\n",
    "    bboxes = tracker(frames, bbox)\n",
    "    visualise_tracking(frames, bboxes)\n",
    "    return\n",
    "\n",
    "gif_path = '/workspace/data/data_folder/o2o/gifs_11'\n",
    "# print(anno[18])\n",
    "track_bbox(anno[11], gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to generate features what we need now is the code to do it.\n",
    "the code works according to x,y,w,h\n",
    "what should be targeted - feature generation code for one gif.\n",
    "the feature is a dictionary. has the following fields\n",
    "metadata, relations, bboxes.\n",
    "\n",
    "image metadata has to be included\n",
    "then there are other keys: relative features, vgg_feature, bbox_features, motion features, i3d features \n",
    "how to go about doing this.\n",
    "\n",
    "All of the relative features need to be generated - like ikea asm.\n",
    "\n",
    "\n",
    "create a function which takes an annotation and generates it's features and returns it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Geometric Feature ( based on bbox dimensions )\n",
    "def geometric_feature(bbox, im_width, im_height):\n",
    "    '''\n",
    "    In Modeling Context Between Objects for Referring Expression Understanding, ECCV 2016\n",
    "    [x_min/W, y_min/H, x_max/W, y_max/H, bbox_area/image_area]\n",
    "    \n",
    "    The annotation are given in Image Coordinate system (X is horizontal & Y is vertical ,(0,0) top left)\n",
    "    The features are calculated in Image Coordinate System as well\n",
    "    '''\n",
    "    x_min = bbox[0]   \n",
    "    y_min = bbox[1]\n",
    "\n",
    "    x_max = bbox[2]\n",
    "    y_max = bbox[3]\n",
    "\n",
    "    bbox_width = x_max - x_min\n",
    "    bbox_height = y_max - y_min\n",
    "    \n",
    "    area_bbox = bbox_width * bbox_height\n",
    "    area_image = im_width * im_height\n",
    "    \n",
    "    feature = [x_min/im_width, y_min/im_height, x_max/im_width, y_max/im_height, area_bbox/area_image]\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    feature = np.asarray(feature, dtype=np.float32)\n",
    "    feature = torch.from_numpy(feature)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2d cnn based deep bbox features\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageFeatureExtractor(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Object feature extractor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, submodule, layer, device, deep_net):\n",
    "    \n",
    "        \"\"\"\n",
    "        input the object detector module and the layer\n",
    "        number on which we want to extract features\n",
    "        \"\"\"\n",
    "        \n",
    "        super(ImageFeatureExtractor, self).__init__()\n",
    "        \n",
    "        self.pretrain_model = submodule\n",
    "        self.layer = layer\n",
    "        \n",
    "        if deep_net == 'resnet50_fpn':\n",
    "    \n",
    "            self.layer_list = list(self.pretrain_model._modules['backbone']._modules['body']._modules.keys())\n",
    "            layer_generator = self.pretrain_model._modules['backbone']._modules['body'].named_children()\n",
    "            self.transform_module = self.pretrain_model._modules['transform']\n",
    "\n",
    "        if deep_net == 'vgg16':\n",
    "    \n",
    "            self.layer_list = list(self.pretrain_model._modules['features']._modules.keys())\n",
    "            layer_generator = self.pretrain_model._modules['features'].named_children()\n",
    "            self.transform_module = None\n",
    "\n",
    "        output_layer = self.layer_list[self.layer]\n",
    "        # just change the number of the layer to get the output\n",
    "        self.children_list = []\n",
    "        \n",
    "\n",
    "        for (name, comp_layer) in layer_generator:\n",
    "            self.children_list.append(comp_layer)\n",
    "            if name == output_layer:\n",
    "                break\n",
    "        \n",
    "        self.feature_extrac_net = nn.Sequential(*self.children_list).to(device)\n",
    "        self.pretrain_model = None\n",
    "        \n",
    "    def forward(self, image):\n",
    "        \n",
    "        if self.transform_module:\n",
    "\n",
    "            transformation = self.transform_module(image)[0]\n",
    "            shape = transformation.image_sizes[0]\n",
    "            transformed_image = transformation.tensors\n",
    "            image = transformed_image[:,:,:shape[0], :shape[1]]\n",
    "        \n",
    "        feature = self.feature_extrac_net(image)\n",
    "\n",
    "        return feature\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Doesn't need correction\n",
    "def extract_image_deep_feature_faster(image, feature_extractor, device):\n",
    "    \n",
    "    image = np.swapaxes(image, 0, 2)\n",
    "    image = np.swapaxes(image, 1, 2)\n",
    "    image = np.expand_dims(image, 0)\n",
    "    \n",
    "    image_tensor = torch.from_numpy(image).to(device)\n",
    "    image_feature = feature_extractor(image_tensor)\n",
    "\n",
    "    return image_feature\n",
    "\n",
    "\n",
    "import torchvision\n",
    "\n",
    "# Doesn't need correction\n",
    "def roi_align(feature_map, boxes):\n",
    "    \n",
    "    pooler = torchvision.ops.RoIAlign(output_size=(7, 7), spatial_scale = 1.0, sampling_ratio=1)\n",
    "    boxes_list = [boxes]\n",
    "    output = pooler(feature_map, boxes_list)\n",
    "\n",
    "    return output\n",
    "\n",
    "import torch.nn.functional as F \n",
    "import torch\n",
    "\n",
    "# Corrected\n",
    "def extract_bbox_deep_features_faster(image, bboxes, im_shape, feature_extractor, device):\n",
    "    '''\n",
    "    bboxes: tensor with bbox coords\n",
    "    '''\n",
    "    image = image.astype('float32')\n",
    "    fmap = extract_image_deep_feature_faster(image, feature_extractor, device)\n",
    "    \n",
    "    im_width_annotation = im_shape[0]\n",
    "    im_height_annotation = im_shape[1]\n",
    "\n",
    "    # boxes_list = []\n",
    "    # num_boxes = int(bboxes.shape[0])\n",
    "    # for i in range(num_boxes):\n",
    "    #     temp_bbox = bboxes[i]\n",
    "    #     boxes_list.append(temp_bbox)\n",
    "\n",
    "    fmap_device = fmap.device\n",
    "\n",
    "    im_height, im_width, _ = image.shape\n",
    "    # im_scale_width, im_scale_height = (im_width*1.0)/im_width_annotation, (im_height*1.0)/im_height_annotation\n",
    "    \n",
    "    _, fmap_height, fmap_width, __ = fmap.shape\n",
    "    fmap_scale_width, fmap_scale_height = (fmap_width*1.0)/im_width_annotation, (fmap_height*1.0)/im_height_annotation\n",
    "    im_size = (im_width, im_height)\n",
    "\n",
    "    # boxes = np.asarray(boxes_list, dtype='float32')\n",
    "    # boxes = torch.from_numpy(boxes)\n",
    "\n",
    "    # print(annotation['objects_coco'])\n",
    "\n",
    "    # scaling of bbox coordinates according to the resized fmap\n",
    "    from copy import copy as copy\n",
    "    boxes = copy(bboxes)\n",
    "    \n",
    "    boxes[:,0] *= fmap_scale_width\n",
    "    boxes[:,2] *= fmap_scale_width\n",
    "    boxes[:,1] *= fmap_scale_height\n",
    "    boxes[:,3] *= fmap_scale_height\n",
    "    \n",
    "    boxes = boxes.to(fmap_device)\n",
    "    \n",
    "    bbox_features = roi_align(fmap, boxes)\n",
    "    bbox_features = F.avg_pool2d(bbox_features, (7,7)).squeeze(2).squeeze(2)\n",
    "        \n",
    "    return bbox_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For mIoU and distance\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Corrected\n",
    "def calculate_iou(box_1, box_2):\n",
    "\n",
    "    '''\n",
    "    boxes in [min_x, min_y, max_x, max_y] format\n",
    "    '''\n",
    "    # if torch.sum(box_1 == box_2) == 4:\n",
    "    # return 1\n",
    "\n",
    "    b1_min_x, b1_min_y = box_1[0], box_1[1]\n",
    "    b1_max_x, b1_max_y = box_1[2], box_2[3]\n",
    "\n",
    "    b2_min_x, b2_min_y = box_2[0], box_2[1]\n",
    "    b2_max_x, b2_max_y = box_2[2], box_2[3]\n",
    "\n",
    "\n",
    "    b1 = [[b1_min_x, b1_min_y], [b1_min_x, b1_max_y], [b1_max_x, b1_max_y], [b1_max_x, b1_min_y]]\n",
    "    b2 = [[b2_min_x, b2_min_y], [b2_min_x, b2_max_y], [b2_max_x, b2_max_y], [b2_max_x, b2_min_y]]\n",
    "\n",
    "    poly_1 = Polygon(b1)\n",
    "    poly_2 = Polygon(b2)\n",
    "\n",
    "    i_area = poly_1.intersection(poly_2).area\n",
    "    u_area = poly_1.union(poly_2).area\n",
    "    \n",
    "    iou = i_area / u_area\n",
    "    \n",
    "    return iou\n",
    "\n",
    "# Corrected\n",
    "def calculate_distance_normalized(box_1, box_2, im_width, im_height):\n",
    "    \n",
    "    '''\n",
    "    boxes in [min_x, min_y, max_x, max_y] format\n",
    "    '''\n",
    "\n",
    "    b1_c_x = (box_1[0] + box_1[2]) * 0.5\n",
    "    b1_c_y = (box_1[1] + box_1[3]) * 0.5\n",
    "\n",
    "    b2_c_x = (box_2[0] + box_2[2]) * 0.5\n",
    "    b2_c_y = (box_2[1] + box_2[3]) * 0.5\n",
    "\n",
    "    b1_x, b1_y = b1_c_x/im_width, b1_c_y/im_height\n",
    "    b2_x, b2_y = b2_c_x/im_width, b2_c_y/im_height\n",
    "    \n",
    "    # normalized distance in 0 to 1\n",
    "    dis = np.sqrt( (b1_x-b2_x)**2 + (b1_y-b2_y)**2 ) / np.sqrt(2)\n",
    "\n",
    "    return dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For relative spatial features\n",
    "import numpy as np\n",
    "import torch\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Corrected\n",
    "def box_deltas(subject_box, object_box):\n",
    "    '''\n",
    "    boxes in [centre_x, centre_y, width, height] format\n",
    "    '''\n",
    "\n",
    "    s_width = subject_box[2] - subject_box[0]\n",
    "    s_height = subject_box[3] - subject_box[1]\n",
    "    \n",
    "    o_width = object_box[2] - object_box[0]\n",
    "    o_height = object_box[3] - object_box[1]\n",
    "\n",
    "    s_centre_x = subject_box[0] + (s_width/2)\n",
    "    s_centre_y = subject_box[1] + (s_height/2)\n",
    "\n",
    "    o_centre_x = object_box[0] + (o_width/2)\n",
    "    o_centre_y = object_box[1] + (o_height/2)\n",
    "    \n",
    "    t_so_x = (s_centre_x - o_centre_x)/s_width\n",
    "    t_so_y = (s_centre_y - o_centre_y)/s_height\n",
    "    \n",
    "    t_so_w = torch.log(s_width/o_width)\n",
    "    t_so_h = torch.log(s_height/o_height)\n",
    "    \n",
    "    t_os_x = (o_centre_x - s_centre_x)/o_width\n",
    "    t_os_y = (o_centre_y - s_centre_y)/o_height\n",
    "    \n",
    "    data = [t_so_x, t_so_y, t_so_w, t_so_h, t_os_x, t_os_y]\n",
    "\n",
    "    return torch.FloatTensor(data)\n",
    "\n",
    "\n",
    "def get_union_box(box_1, box_2):\n",
    "\n",
    "    '''\n",
    "    boxes in [min_x, min_y, max_x, max_y] format\n",
    "    '''\n",
    "\n",
    "    b1_min_x, b1_min_y = box_1[0], box_1[1]\n",
    "    b1_max_x, b1_max_y = box_1[2], box_2[3]\n",
    "\n",
    "    b2_min_x, b2_min_y = box_2[0], box_2[1]\n",
    "    b2_max_x, b2_max_y = box_2[2], box_2[3]\n",
    "\n",
    "    bu_min_x, bu_min_y = min(b1_min_x, b2_min_x), min(b1_min_y, b2_min_y)\n",
    "    bu_max_x, bu_max_y = max(b1_max_x, b2_max_x), max(b1_max_y, b2_max_y)\n",
    "  \n",
    "    return [bu_min_x, bu_min_y, bu_max_x, bu_max_y]\n",
    "\n",
    "def calculate_distance(box_1, box_2):\n",
    "    \n",
    "    '''\n",
    "    boxes in [min_x, min_y, max_x, max_y] format\n",
    "    '''\n",
    "\n",
    "    b1_c_x = (box_1[0] + box_1[2]) * 0.5\n",
    "    b1_c_y = (box_1[1] + box_1[3]) * 0.5\n",
    "\n",
    "    b2_c_x = (box_2[0] + box_2[2]) * 0.5\n",
    "    b2_c_y = (box_2[1] + box_2[3]) * 0.5\n",
    "\n",
    "    dis = np.sqrt( (b1_c_x-b2_c_x)**2 + (b1_c_y-b2_c_y)**2 )\n",
    "\n",
    "    return dis\n",
    "\n",
    "# Corrected\n",
    "def relative_spatial_features_old(image_annotation, pad_dimension=15):\n",
    "    \n",
    "    bbox_coordinates = []\n",
    "    num_objects = len(image_annotation['objects_coco'])\n",
    "\n",
    "    im_height = image_annotation['image_metadata']['height']\n",
    "    im_width = image_annotation['image_metadata']['width']\n",
    "\n",
    "\n",
    "    for obj in image_annotation['objects_coco']:\n",
    "\n",
    "        # Normalizing bbox coordinates    \n",
    "\n",
    "        x0, y0, x1, y1 = obj['bbox']\n",
    "\n",
    "        x0_n = x0/im_width\n",
    "        y0_n = y0/im_height\n",
    "        x1_n = x1/im_width\n",
    "        y1_n = y1/im_height\n",
    "        \n",
    "        bbox_coordinates.append([x0_n, y0_n, x1_n, y1_n])\n",
    "    \n",
    "    bbox_coordinates = torch.from_numpy(np.asarray(bbox_coordinates, dtype='float32'))\n",
    "    relative_features = torch.zeros(pad_dimension, pad_dimension, 20, dtype=torch.float32)\n",
    "    \n",
    "    for i in range(num_objects):\n",
    "        for j in range(num_objects):\n",
    "\n",
    "            # To make the edge feature matrix symmetric\n",
    "            if (i<=j):\n",
    "                subject_box = bbox_coordinates[i]\n",
    "                object_box = bbox_coordinates[j]\n",
    "                union_box = get_union_box(subject_box, object_box)\n",
    "            else:\n",
    "                subject_box = bbox_coordinates[j]\n",
    "                object_box = bbox_coordinates[i]\n",
    "                union_box = get_union_box(subject_box, object_box)\n",
    "\n",
    "            relative_features[i,j,:6] = box_deltas(subject_box=subject_box, object_box=object_box)\n",
    "            relative_features[i,j,6:12] = box_deltas(subject_box=subject_box, object_box=union_box)\n",
    "            relative_features[i,j,12:18] = box_deltas(subject_box=object_box, object_box=union_box)\n",
    "            relative_features[i,j,18] = calculate_iou(subject_box, object_box)\n",
    "            # check implementation for semantics of div = 1 \n",
    "            relative_features[i,j,19] = calculate_distance_normalized(subject_box, object_box)\n",
    "    \n",
    "    return relative_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Corrected\n",
    "def relative_spatial_features(bbox_1, bbox_2, im_width, im_height):\n",
    "    \n",
    "    bbox_1[0]/=im_width\n",
    "    bbox_1[2]/=im_width\n",
    "    bbox_1[1]/=im_height\n",
    "    bbox_1[3]/=im_height\n",
    "\n",
    "    bbox_2[0]/=im_width\n",
    "    bbox_2[2]/=im_width\n",
    "    bbox_2[1]/=im_height\n",
    "    bbox_2[3]/=im_height\n",
    "    \n",
    "    relative_features = torch.zeros(20, dtype=torch.float32)\n",
    "    \n",
    "    subject_box = bbox_1\n",
    "    object_box = bbox_2\n",
    "\n",
    "    union_box = get_union_box(subject_box, object_box)\n",
    "\n",
    "    relative_features[:6] = box_deltas(subject_box=subject_box, object_box=object_box)\n",
    "    relative_features[6:12] = box_deltas(subject_box=subject_box, object_box=union_box)\n",
    "    relative_features[12:18] = box_deltas(subject_box=object_box, object_box=union_box)\n",
    "    relative_features[18] = calculate_iou(subject_box, object_box)\n",
    "    relative_features[19] = calculate_distance(subject_box, object_box)\n",
    "    \n",
    "    return relative_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/transforms/_functional_video.py:5: UserWarning: The _functional_video module is deprecated. Please use the functional module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/transforms/_transforms_video.py:25: UserWarning: The _transforms_video module is deprecated. Please use the transforms module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# For i3d based bbox features\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "import pytorchvideo.models as models\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self, submodule, layer):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.pretrain_model = submodule\n",
    "        self.layer = layer\n",
    "        \n",
    "        self.layer_list = list(self.pretrain_model._modules['blocks']._modules.keys())\n",
    "        print(list(self.pretrain_model._modules['blocks']._modules))\n",
    "        output_layer = self.layer_list[self.layer]  # just change the number of the layer to get the output\n",
    "\n",
    "        self.children_list = []\n",
    "        for (name, comp_layer) in self.pretrain_model._modules['blocks'].named_children():\n",
    "            self.children_list.append(comp_layer)\n",
    "            if name == output_layer:\n",
    "                break\n",
    "        #print(self.children_list)\n",
    "        self.feature_extrac_net = nn.Sequential(*self.children_list)\n",
    "        self.pretrain_model = None\n",
    "\n",
    "    def forward(self, image):\n",
    "        feature = self.feature_extrac_net(image)\n",
    "        return feature\n",
    "\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import  NormalizeVideo\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    UniformTemporalSubsample\n",
    ")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def read_gif(gif_path):\n",
    "    \"\"\"read gif and return dictionary as key ['video'] and value the tensor of size(CxTxHXW)\"\"\"\n",
    "\n",
    "    video = EncodedVideo.from_path(gif_path)\n",
    "    video = video.get_clip(0, 5) # get_clip fetches the clip from starting time to ending time\n",
    "\n",
    "    return video\n",
    "\n",
    "# Doesn't need correction\n",
    "def roi_align_i3d(feature_map, boxes):\n",
    "    \n",
    "    pooler = torchvision.ops.RoIAlign(output_size=(1, 1), spatial_scale = 1.0, sampling_ratio=1)\n",
    "    boxes_list = [boxes]\n",
    "    output = pooler(feature_map, boxes_list)\n",
    "\n",
    "    return output\n",
    "\n",
    "def roi_align_custom(feature_map, boxes, im_width, im_height):\n",
    "    '''\n",
    "    feature_map : [B,C,T,H,W] B - Batch size (expected 1)\n",
    "    boxes: [N, T, 4] N is number of objects\n",
    "    '''\n",
    "    \n",
    "    fmap_height, fmap_width = feature_map.shape[3:]\n",
    "    boxes[0]/=im_width\n",
    "    boxes[2]/=im_width\n",
    "\n",
    "    boxes[1]/=im_height\n",
    "    boxes[3]/=im_height\n",
    "\n",
    "    boxes[0]*=fmap_width\n",
    "    boxes[2]*=fmap_width\n",
    "\n",
    "    boxes[1]/=fmap_height\n",
    "    boxes[3]/=fmap_height\n",
    "    # output['bboxes'] = torch.zeros(max_num_obj,len(frames),4, dtype=torch.float)\n",
    "    # uniform temporal subsample selects 1,2,3,4,5,6,7,8,9,11\n",
    "    boxes = boxes[:,0:9:2,:]\n",
    "    time_steps = boxes.shape[1]\n",
    "    \n",
    "    roi_align_res = []\n",
    "    for t in range(time_steps):\n",
    "        temp_fmap = feature_map[:,:,t,:,:]\n",
    "        temp_boxes = boxes[:,t,:]\n",
    "        temp_res = roi_align_i3d(temp_fmap, temp_boxes)\n",
    "        roi_align_res.append(temp_res)\n",
    "    \n",
    "    return roi_align_res\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For motion feature calculation\n",
    "\n",
    "def calculate_motion_feature(geom_feat_1, geom_feat_2):\n",
    "    return geom_feat_1 - geom_feat_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master Feature Generator\n",
    "import torch\n",
    "def master_feature_generator(annotation, gif_folder, cnn_feature_extractor, i3d_feature_extractor, i3d_transform, device):\n",
    "\n",
    "    # Getting details to load the GIF\n",
    "    yt_id = annotation['metadata']['yt_id']\n",
    "    frame_index = annotation['metadata']['frame no.']\n",
    "\n",
    "    temp = int(int(gif_folder.split('_')[-1])/2)\n",
    "    window_size = temp\n",
    "\n",
    "    # Loading the gif    \n",
    "    # getting the file location\n",
    "    filename = yt_id + '_' + str(frame_index) + '_' + str(window_size) + '.gif'\n",
    "    import os\n",
    "    file_location = os.path.join(gif_folder, filename)\n",
    "    import cv2\n",
    "\n",
    "    # getting the frames\n",
    "    vid = cv2.VideoCapture(file_location)\n",
    "    frames = []\n",
    "    frame_count = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    for i in range(frame_count):\n",
    "        success, frame = vid.read()\n",
    "        frames.append(frame)\n",
    "    central_frame = frames[window_size]\n",
    "\n",
    "    # Sanity Check    \n",
    "    assert window_size == (len(frames) - 1)/2, \"Possible issue, please check\"\n",
    "\n",
    "    # Output Dictionary\n",
    "    output = {}\n",
    "    output['legend'] = {}\n",
    "    \n",
    "    # Adding the metadata\n",
    "    output['metadata'] = annotation['metadata']\n",
    "    im_height, im_width, _ = frames[0].shape    # NumPy has num rows, num cols which is height and width according to opencv conventions\n",
    "    output['metadata']['frame_width'] = im_width\n",
    "    output['metadata']['frame_height'] = im_height\n",
    "    \n",
    "    # total num of annotated objects \n",
    "    output['num_obj'] = len(list(annotation['bboxes'].keys()))\n",
    "\n",
    "    # saving bbox co-ordinates of objects according to their key in bboxes field\n",
    "    max_num_obj = 12\n",
    "\n",
    "    # bounding box coordinates. not normalized. for image width and height see the metadata\n",
    "    output['bboxes'] = torch.zeros(max_num_obj,len(frames),4, dtype=torch.float)\n",
    "    bbox_keys = annotation['bboxes'].keys()\n",
    "    \n",
    "    for key in bbox_keys:\n",
    "        key_val = int(key)\n",
    "        temp_bbox = annotation['bboxes'][key]['bbox']\n",
    "        \n",
    "        tracked_bboxes = tracker(frames, temp_bbox)\n",
    "        tracked_bboxes = torch.from_numpy(np.asarray(tracked_bboxes, dtype=float))\n",
    "        output['bboxes'][key_val,:,:] = tracked_bboxes\n",
    "\n",
    "\n",
    "    # saving relations in tensors\n",
    "\n",
    "    # maps to transform text to indices\n",
    "    cr_map = {'Contact': 0, 'No Contact': 1, 'None of these': 2, '': 2}\n",
    "    lr_map = {'Below/Above': 0, 'Behind/Front': 1, 'Left/Right': 2, 'Inside': 3, 'None of these': 4, '': 4}\n",
    "    mr_map = {'Holding': 0, 'Carrying': 1, 'Adjusting': 2, 'Rubbing': 3, 'Sliding': 4, 'Rotating': 5, 'Twisting': 6,\n",
    "              'Raising': 7, 'Lowering': 8, 'Penetrating': 9, 'Moving Toward': 10, 'Moving Away': 11, \n",
    "              'Negligible Relative Motion': 12, 'None of these': 13, '': 13}\n",
    "\n",
    "    max_num_rels = 15\n",
    "\n",
    "    # tensor storing relations between objects at the corresponding index in object_pairs key\n",
    "    output['lr'] = torch.zeros(max_num_rels, 5)\n",
    "    output['mr'] = torch.zeros(max_num_rels, 14)\n",
    "    output['cr'] = torch.zeros(max_num_rels, 3)\n",
    "    \n",
    "    # object indices between which the corresponding relation is annotated\n",
    "    output['object_pairs'] = torch.zeros(max_num_rels, 2)\n",
    "    \n",
    "    # reading relations and saving them to the tensors\n",
    "    for i, rel in enumerate(annotation['relations']):\n",
    "\n",
    "        object_pairs = rel[0]\n",
    "\n",
    "        mr = rel[1]['mr']\n",
    "        lr = rel[1]['lr']\n",
    "        cr = rel[1]['scr']\n",
    "\n",
    "        for r in mr:\n",
    "            temp_val = mr_map[r]\n",
    "            output['mr'][i, temp_val] = 1\n",
    "        for r in lr:\n",
    "            temp_val = lr_map[r]\n",
    "            output['lr'][i, temp_val] = 1\n",
    "        for r in cr:\n",
    "            temp_val = cr_map[r]\n",
    "            output['cr'][i, temp_val] = 1\n",
    "\n",
    "        output['object_pairs'][i] = torch.from_numpy(np.asarray(object_pairs,dtype=float))\n",
    "\n",
    "    # total number of relations and hence the total number of object pairs as well\n",
    "    output['num_relation'] = len(annotation['relations'])\n",
    "\n",
    "    # Now we have bounding boxes, metadata, relations, number of objects, number of relations\n",
    "    \n",
    "    # image features - cnn features for bboxes, bbox coordinate based features, relative feature, miou, distance,\n",
    "    \n",
    "    # bbox coordinate based features\n",
    "    output['geometric_feature'] = torch.zeros(max_num_obj, len(frames), 5, dtype=float)\n",
    "    \n",
    "    for f in range(len(frames)):\n",
    "        for i in range( int(output['num_obj']) ):\n",
    "            temp_bbox = output['bboxes'][i, f]\n",
    "            output['geometric_feature'][i, f] = geometric_feature(temp_bbox, im_width, im_height)\n",
    "\n",
    "    # 2d cnn based features for the bounding boxes of central frame\n",
    "    output['cnn_bbox_feature'] = torch.zeros(max_num_obj, 64, dtype=float)\n",
    "    \n",
    "    # window_size is also the index of the central frame\n",
    "    central_frame_bboxes = output['bboxes'][window_size, :output['num_obj']]\n",
    "    temp = extract_bbox_deep_features_faster(central_frame, central_frame_bboxes, [im_width, im_height], cnn_feature_extractor, device )\n",
    "    output['cnn_bbox_feature'][:output['num_obj'],:] = temp\n",
    "\n",
    "    # miou and distance of bounding boxes\n",
    "    output['iou'] = torch.zeros(max_num_obj, max_num_obj, len(frames))\n",
    "\n",
    "    for f in range(len(frames)):\n",
    "        for i in range( int(output['num_obj']) ):\n",
    "                for j in range( int(output['num_obj']) ):\n",
    "                    \n",
    "                    temp_box_1 = output['bboxes'][i, f]\n",
    "                    temp_box_2 = output['bboxes'][j, f]\n",
    "                    output['iou'][i, j, f] = calculate_iou(temp_box_1, temp_box_2)\n",
    "                        \n",
    "\n",
    "    output['distance'] = torch.zeros(max_num_obj, max_num_obj, len(frames))\n",
    "\n",
    "    for f in range(len(frames)):\n",
    "        for i in range( int(output['num_obj']) ):\n",
    "                for j in range( int(output['num_obj']) ):\n",
    "\n",
    "                    temp_box_1 = output['bboxes'][i, f]\n",
    "                    temp_box_2 = output['bboxes'][j, f]\n",
    "                    output['distance'][i, j, f] = calculate_distance_normalized(temp_box_1, temp_box_2, im_width, im_height)\n",
    "    \n",
    "    # relative features\n",
    "    output['relative_spatial_feature'] = torch.zeros(max_num_obj, max_num_obj, len(frames), 20, dtype=float)\n",
    "\n",
    "    for f in range(len(frames)):\n",
    "        for i in range( int(output['num_obj']) ):\n",
    "                for j in range( int(output['num_obj']) ):\n",
    "                    \n",
    "                    if i<j:\n",
    "                        temp_box_1 = output['bboxes'][i, f]\n",
    "                        temp_box_2 = output['bboxes'][j, f]\n",
    "\n",
    "                    # To keep the features symmetric\n",
    "\n",
    "                    if i>=j:\n",
    "                        temp_box_2 = output['bboxes'][i, f]\n",
    "                        temp_box_1 = output['bboxes'][j, f]\n",
    "\n",
    "                    output['relative_spatial_feature'][i, j, f] = relative_spatial_features(temp_box_1, temp_box_2, im_width, im_height)\n",
    "    \n",
    "    # video features - i3d features, motion features, others? \n",
    "    \n",
    "    # i3d features\n",
    "    temp_i3d_video = read_gif(file_location)\n",
    "    temp_i3d_video = i3d_transform(temp_i3d_video)[\"video\"]\n",
    "    temp_i3d_video = temp_i3d_video.unsqueeze(0).to(device)\n",
    "    \n",
    "    temp_i3d_feature_map = i3d_feature_extractor(temp_i3d_video)\n",
    "    temp_bboxes = output['bboxes'].to(device)\n",
    "    res_i3d_feature_map = roi_align_custom(temp_i3d_feature_map, temp_bboxes, im_width, im_height)\n",
    "    output['i3d_feature_map'] = torch.zeros(window_size, 12, 1024)\n",
    "    \n",
    "    for i, f in enumerate(res_i3d_feature_map):\n",
    "        output['i3d_feature_map'][i] = f[:, :, 0, 0]\n",
    "    \n",
    "    \n",
    "    # motion features\n",
    "    output['motion_feature'] = torch.zeros(max_num_obj, len(frames), 5)\n",
    "\n",
    "    for f in range(len(frames)):\n",
    "        for i in range( int(output['num_obj']) ):\n",
    "                    \n",
    "                    temp_geom_feat_1 = output['geometric_feature'][i, f, :]\n",
    "                    if f == 0:\n",
    "                        temp_geom_feat_2 = 0\n",
    "                    else:\n",
    "                        temp_geom_feat_1 = output['geometric_feature'][i, f-1, :]\n",
    "                    \n",
    "                    output['motion_feature'][i, f, :] = calculate_motion_feature(temp_geom_feat_1, temp_geom_feat_2)\n",
    "\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6']\n"
     ]
    }
   ],
   "source": [
    "# Loading files required to generate the trackers\n",
    "# Load the annotation file\n",
    "anno_path = '/workspace/work/O2ONet/data/annotations_minus_unavailable_yt_vids.pkl'\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "f = open(anno_path, 'rb')\n",
    "annotations = pkl.load(f)\n",
    "f.close()\n",
    "\n",
    "gif_folder = '/workspace/data/data_folder/o2o/gifs_11'\n",
    "\n",
    "# 2d cnn feature extractor\n",
    "import torchvision\n",
    "device = torch.device('cuda:3') if torch.cuda.is_available() else torch.device('cpu')\n",
    "deep_net = 'vgg16'\n",
    "layer_no = 4\n",
    "if deep_net == 'vgg16':\n",
    "    model = torchvision.models.vgg16(pretrained=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "cnn_feature_extractor = ImageFeatureExtractor(model, layer_no, device, deep_net)\n",
    "\n",
    "\n",
    "# i3d feature extractor\n",
    "device = torch.device('cuda:3') if torch.cuda.is_available() else torch.device('cpu')\n",
    "import pytorchvideo.models as models\n",
    "model_name = \"i3d_r50\"\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo:main\", model=model_name, pretrained=True)\n",
    "model = model.to(device)\n",
    "i3d_feature_net = FeatureExtractor(model, 4)\n",
    "\n",
    "# i3d transform\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "i3d_transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(11),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std)    \n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generating all features\n",
    "from tqdm import tqdm as tqdm\n",
    "import os\n",
    "\n",
    "feature_folder = '/workspace/data/data_folder/o2o/gifs_11_features'\n",
    "os.makedirs(feature_folder, exist_ok=True)\n",
    "\n",
    "issues = []\n",
    "i = 0\n",
    "\n",
    "for annotation in tqdm(annotations):\n",
    "    \n",
    "    i+=1\n",
    "    # generating location to save the feature dictionary\n",
    "    yt_id = annotation['metadata']['yt_id']\n",
    "    frame_index = annotation['metadata']['frame no.']\n",
    "    window_size = int(int(gif_folder.split('_')[-1])/2)\n",
    "    filename = yt_id + '_' + str(frame_index) + '_' + str(window_size) + '.pt'\n",
    "    file_location = os.path.join(feature_folder, filename)\n",
    "\n",
    "    if os.path.exists(file_location):\n",
    "        continue\n",
    "    \n",
    "    # generating feature dictionary\n",
    "    try:\n",
    "        feature_dict = master_feature_generator(annotation, gif_folder, cnn_feature_extractor, i3d_feature_net, i3d_transform, device)\n",
    "    except:\n",
    "        print(\" Issue in \",i)\n",
    "        issues.append(i)\n",
    "        continue\n",
    "    \n",
    "    # saving the feature dictionary\n",
    "    torch.save(feature_dict, file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48, 82, 318, 638, 1533, 1841]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For combining all features into one pickle file\n",
    "\n",
    "def combine_all_pickles(feature_folder):\n",
    "    from glob import glob as glob\n",
    "    files = glob(feature_folder + '/*.pt')\n",
    "    \n",
    "    all_features = []\n",
    "    import torch\n",
    "    \n",
    "    for f in files:\n",
    "        data = torch.load(f)\n",
    "        all_features.append(data)\n",
    "    return all_features\n",
    "\n",
    "feature_folder = '/workspace/data/data_folder/o2o/gifs_11_features'\n",
    "data = combine_all_pickles(feature_folder)\n",
    "\n",
    "# saving_loc = '/workspace/data/data_folder/o2o/all_features/gifs_11/all_features_11.pkl'\n",
    "\n",
    "# import pickle as pickle\n",
    "# f = open(saving_loc, 'wb')\n",
    "# pickle.dump(data, f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2050"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating train, test and val splits using previously generated data\n",
    "\n",
    "# train, test, validation split\n",
    "\n",
    "import pickle\n",
    "\n",
    "folder_loc = '/workspace/work/CVPR22/ooi_classification/hidden/gcn/data'\n",
    "\n",
    "train_loc = folder_loc + '/training3.pkl'\n",
    "test_loc = folder_loc + '/testing3.pkl'\n",
    "val_loc = folder_loc + '/validation3.pkl'\n",
    "\n",
    "train_data = pickle.load(open(train_loc,'rb'))\n",
    "val_data = pickle.load(open(val_loc,'rb'))\n",
    "test_data = pickle.load(open(test_loc,'rb'))\n",
    "\n",
    "\n",
    "split_dict = {}\n",
    "\n",
    "for t in train_data:\n",
    "    yt_id = t['metadata']['yt_id']\n",
    "    frame_no = t['metadata']['frame no.']\n",
    "    temp_key = yt_id + '_' + frame_no\n",
    "    split_dict[temp_key] = 'train'\n",
    "    \n",
    "for t in test_data:\n",
    "    yt_id = t['metadata']['yt_id']\n",
    "    frame_no = t['metadata']['frame no.']\n",
    "    temp_key = yt_id + '_' + frame_no\n",
    "    split_dict[temp_key] = 'test'\n",
    "    \n",
    "for t in val_data:\n",
    "    yt_id = t['metadata']['yt_id']\n",
    "    frame_no = t['metadata']['frame no.']\n",
    "    temp_key = yt_id + '_' + frame_no\n",
    "    split_dict[temp_key] = 'val'\n",
    "    \n",
    "saving_path = '/workspace/data/data_folder/o2o/split_dict.pkl'\n",
    "\n",
    "pickle.dump(split_dict, open(saving_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For splitting combined features file into 3 pickle files using split dictionary\n",
    "import pickle as pkl\n",
    "\n",
    "def split(split_dict_path, combined_features_file):\n",
    "\n",
    "\n",
    "    all_features = pkl.load(open(combined_features_file, 'rb'))\n",
    "    split_dict = pkl.load(open(split_dict_path,'rb'))\n",
    "    \n",
    "    train = []\n",
    "    test = []\n",
    "    val = []\n",
    "    \n",
    "    for feat in all_features:\n",
    "        yt_id = feat['metadata']['yt_id']\n",
    "        frame_no = feat['metadata']['frame no.']\n",
    "        temp_key = yt_id + '_' + frame_no\n",
    "        split = split_dict[temp_key]\n",
    "        \n",
    "        if split == 'train':\n",
    "            train.append(feat)\n",
    "        elif split == 'test':\n",
    "            test.append(feat)\n",
    "        elif split == 'val':\n",
    "            val.append(feat)\n",
    "\n",
    "            \n",
    "\n",
    "    saving_folder = '/'.join( combined_features_file.split('/')[:-1] )\n",
    "    import os\n",
    "    \n",
    "    train_file = os.path.join(saving_folder, 'train.pkl')\n",
    "    f = open(train_file, 'wb')\n",
    "    pkl.dump(train, f)\n",
    "    f.close()\n",
    "    \n",
    "    test_file = os.path.join(saving_folder, 'test.pkl')\n",
    "    f = open(test_file, 'wb')\n",
    "    pkl.dump(test, f)\n",
    "    f.close()\n",
    "\n",
    "    val_file = os.path.join(saving_folder, 'val.pkl')\n",
    "    f = open(val_file, 'wb')\n",
    "    pkl.dump(val, f)\n",
    "    f.close()\n",
    "    \n",
    "    return train, test, val\n",
    "\n",
    "split_dict_path = '/workspace/data/data_folder/o2o/split_dict.pkl'\n",
    "combined_features_file = '/workspace/data/data_folder/o2o/all_features/gifs_11/all_features_11.pkl'\n",
    "\n",
    "train, test, val = split(split_dict_path, combined_features_file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
