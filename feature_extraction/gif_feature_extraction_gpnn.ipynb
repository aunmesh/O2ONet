{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "def roi_align(feature_map, boxes, num_obj, im_width, im_height):\n",
    "\n",
    "    feature_map = feature_map.to(torch.device('cpu'))\n",
    "\n",
    "    fmap_height, fmap_width = feature_map.shape[2:]\n",
    "    boxes[:,:,0]/=im_width\n",
    "    boxes[:,:,2]/=im_width\n",
    "\n",
    "    boxes[:,:,1]/=im_height\n",
    "    boxes[:,:,3]/=im_height\n",
    "\n",
    "    boxes[:,:,0]*=fmap_width\n",
    "    boxes[:,:,2]*=fmap_width\n",
    "\n",
    "    boxes[:,:,1]/=fmap_height\n",
    "    boxes[:,:,3]/=fmap_height\n",
    "    \n",
    "    pooler = torchvision.ops.RoIAlign(output_size=(1, 1), spatial_scale = 1.0, sampling_ratio=1)\n",
    "    \n",
    "    \n",
    "    num_frames = 11\n",
    "    roi_pools = []\n",
    "\n",
    "    boxes_list = list(boxes.split(1, dim=1))\n",
    "    boxes_list = [boxes_list[i].squeeze(1) for i in range(len(boxes_list))]\n",
    "    \n",
    "    boxes_list_2 = []\n",
    "    \n",
    "    for i, b in enumerate(boxes_list):\n",
    "        boxes_list_2.append( b[:num_obj] )\n",
    "    \n",
    "    pooled_output = pooler(feature_map, boxes_list_2)\n",
    "    return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master Feature Generator for VSGNet\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def master_feature_generator_old( full_feature_folder, old_data_dict):\n",
    "\n",
    "    # Get the feature from the full feature folder\n",
    "    # Load the resnet 152 feature map\n",
    "    # remove unnecessary keys\n",
    "    # perform RoIalign on the resnet152 feature map\n",
    "    # return the result\n",
    "    \n",
    "    yt_id = old_data_dict['metadata']['yt_id']\n",
    "    frame_no = old_data_dict['metadata']['frame no.']\n",
    "    suffix = '5'\n",
    "    \n",
    "    full_feature_filename = yt_id + '_' + str(frame_no) + '_' + suffix + '.pt'\n",
    "    vsgnet_feature_file_loc = os.path.join( full_feature_folder, full_feature_filename)\n",
    "    \n",
    "    full_feature = torch.load(vsgnet_feature_file_loc)\n",
    "    \n",
    "    convo_feature_map = full_feature['i3d_fmap']\n",
    "    bboxes = full_feature['bboxes']\n",
    "    \n",
    "    old_data_dict['bboxes'] = bboxes # as old_data_dict bboxes are corrupt\n",
    "    \n",
    "    del full_feature\n",
    "    \n",
    "    im_width = old_data_dict['metadata']['frame_width']\n",
    "    im_height = old_data_dict['metadata']['frame_height']\n",
    "    \n",
    "    num_obj = old_data_dict['num_obj']\n",
    "    \n",
    "    # perform RoI align\n",
    "    roialign_output = roi_align( convo_feature_map, bboxes, num_obj, im_width, im_height)\n",
    "    \n",
    "    result_tensor = torch.zeros((11, 12, 1024))\n",
    "    \n",
    "    for frame in range(11):\n",
    "        ind0 = frame*num_obj\n",
    "        ind1 = (frame + 1)*num_obj\n",
    "        result_tensor[frame, :num_obj] = roialign_output[ind0:ind1,0,0]\n",
    "        \n",
    "    keep_keys = ['metadata', 'num_obj', 'lr', 'mr', 'cr', 'object_pairs', 'num_relation',\n",
    "                 'i3d_feature_map', 'bboxes']\n",
    "    \n",
    "    new_dict = {}\n",
    "    \n",
    "    for k in keep_keys:\n",
    "        new_dict[k] = old_data_dict[k]\n",
    "    \n",
    "    new_dict['cnn_bbox_feature'] = result_tensor\n",
    "        \n",
    "    return new_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def master_feature_generator( gpnn_feature_dict, old_data_dict):\n",
    "\n",
    "    \n",
    "    yt_id = old_data_dict['metadata']['yt_id']\n",
    "    frame_no = old_data_dict['metadata']['frame no.']\n",
    "\n",
    "    yt_id_gpnn = gpnn_feature_dict['metadata']['yt_id']\n",
    "    frame_no_gpnn = gpnn_feature_dict['metadata']['frame no.']\n",
    "    \n",
    "    assert yt_id_gpnn==yt_id and frame_no==frame_no_gpnn, \"Some Issue\"\n",
    "    \n",
    "\n",
    "    gpnn_feature_dict['relative_spatial_feature'] = old_data_dict['relative_spatial_feature']\n",
    "    return gpnn_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug torch.Size([12, 4])\n"
     ]
    }
   ],
   "source": [
    "# vsgnet_feature_folder = '/workspace/data/data_folder/o2o/gifs_11_features_vsgnet'\n",
    "\n",
    "# import torch\n",
    "\n",
    "# temp_old_data_loc = '/workspace/data/data_folder/o2o/gifs_11_features/_1_dgZ4-Ldw_2432_5.pt'\n",
    "# old_data_dict = torch.load(temp_old_data_loc)\n",
    "\n",
    "# roi_align_res = master_feature_generator(vsgnet_feature_folder, old_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:00<00:00, 197582.12it/s]\n"
     ]
    }
   ],
   "source": [
    "split = 'val'\n",
    "\n",
    "features_file = '/workspace/data/data_folder/o2o/all_features/gifs_11/' + split + '.pkl'\n",
    "gpnn_feature_file = '/workspace/data/data_folder/o2o/all_features/gpnn/' + split + '.pkl'\n",
    "\n",
    "import pickle as pkl\n",
    "features = pkl.load(open(features_file,'rb'))\n",
    "features_gpnn = pkl.load(open(gpnn_feature_file,'rb'))\n",
    "\n",
    "new_features = []\n",
    "\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "for i in tqdm(range(len(features))):\n",
    "    old_dict = features[i]\n",
    "    gpnn_dict = features_gpnn[i]\n",
    "    \n",
    "    new_dict = master_feature_generator(gpnn_dict, old_dict)\n",
    "    new_features.append(new_dict)\n",
    "\n",
    "new_features_file = open('/workspace/data/data_folder/o2o/all_features/gpnn_with_edge/' + split + '.pkl','wb')\n",
    "\n",
    "pkl.dump(new_features, new_features_file)\n",
    "new_features_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = master_feature_generator(vsgnet_feature_folder, features[346])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['metadata', 'num_obj', 'lr', 'mr', 'cr', 'object_pairs', 'num_relation', 'i3d_feature_map', 'bboxes', 'cnn_bbox_feature'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi_align_res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For combining all features into one pickle file\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "def combine_all_pickles(feature_folder):\n",
    "    from glob import glob as glob\n",
    "    files = glob(feature_folder + '/*.pt')\n",
    "    \n",
    "    all_features = []\n",
    "    import torch\n",
    "    \n",
    "    for f in tqdm(files):\n",
    "        data = torch.load(f)\n",
    "        all_features.append(data)\n",
    "    return all_features\n",
    "\n",
    "feature_folder = '/workspace/data/data_folder/o2o/gifs_11_features_vsgnet'\n",
    "data = combine_all_pickles(feature_folder)\n",
    "\n",
    "saving_loc = '/workspace/data/data_folder/o2o/all_features/gifs_11_vsgnet/all_features_11_vsgnet.pkl'\n",
    "\n",
    "import pickle as pickle\n",
    "f = open(saving_loc, 'wb')\n",
    "pickle.dump(data, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2050"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating train, test and val splits using previously generated data\n",
    "\n",
    "# train, test, validation split\n",
    "\n",
    "import pickle\n",
    "\n",
    "folder_loc = '/workspace/work/CVPR22/ooi_classification/hidden/gcn/data'\n",
    "\n",
    "train_loc = folder_loc + '/training3.pkl'\n",
    "test_loc = folder_loc + '/testing3.pkl'\n",
    "val_loc = folder_loc + '/validation3.pkl'\n",
    "\n",
    "train_data = pickle.load(open(train_loc,'rb'))\n",
    "val_data = pickle.load(open(val_loc,'rb'))\n",
    "test_data = pickle.load(open(test_loc,'rb'))\n",
    "\n",
    "\n",
    "split_dict = {}\n",
    "\n",
    "for t in train_data:\n",
    "    yt_id = t['metadata']['yt_id']\n",
    "    frame_no = t['metadata']['frame no.']\n",
    "    temp_key = yt_id + '_' + frame_no\n",
    "    split_dict[temp_key] = 'train'\n",
    "    \n",
    "for t in test_data:\n",
    "    yt_id = t['metadata']['yt_id']\n",
    "    frame_no = t['metadata']['frame no.']\n",
    "    temp_key = yt_id + '_' + frame_no\n",
    "    split_dict[temp_key] = 'test'\n",
    "    \n",
    "for t in val_data:\n",
    "    yt_id = t['metadata']['yt_id']\n",
    "    frame_no = t['metadata']['frame no.']\n",
    "    temp_key = yt_id + '_' + frame_no\n",
    "    split_dict[temp_key] = 'val'\n",
    "    \n",
    "saving_path = '/workspace/data/data_folder/o2o/split_dict.pkl'\n",
    "\n",
    "pickle.dump(split_dict, open(saving_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For splitting combined features file into 3 pickle files using split dictionary\n",
    "import pickle as pkl\n",
    "\n",
    "def split(split_dict_path, combined_features_file):\n",
    "\n",
    "\n",
    "    all_features = pkl.load(open(combined_features_file, 'rb'))\n",
    "    split_dict = pkl.load(open(split_dict_path,'rb'))\n",
    "    \n",
    "    train = []\n",
    "    test = []\n",
    "    val = []\n",
    "    \n",
    "    for feat in all_features:\n",
    "        yt_id = feat['metadata']['yt_id']\n",
    "        frame_no = feat['metadata']['frame no.']\n",
    "        temp_key = yt_id + '_' + frame_no\n",
    "        split = split_dict[temp_key]\n",
    "        \n",
    "        if split == 'train':\n",
    "            train.append(feat)\n",
    "        elif split == 'test':\n",
    "            test.append(feat)\n",
    "        elif split == 'val':\n",
    "            val.append(feat)\n",
    "\n",
    "            \n",
    "\n",
    "    saving_folder = '/'.join( combined_features_file.split('/')[:-1] )\n",
    "    import os\n",
    "    \n",
    "    train_file = os.path.join(saving_folder, 'train.pkl')\n",
    "    f = open(train_file, 'wb')\n",
    "    pkl.dump(train, f)\n",
    "    f.close()\n",
    "    \n",
    "    test_file = os.path.join(saving_folder, 'test.pkl')\n",
    "    f = open(test_file, 'wb')\n",
    "    pkl.dump(test, f)\n",
    "    f.close()\n",
    "\n",
    "    val_file = os.path.join(saving_folder, 'val.pkl')\n",
    "    f = open(val_file, 'wb')\n",
    "    pkl.dump(val, f)\n",
    "    f.close()\n",
    "    \n",
    "    return train, test, val\n",
    "\n",
    "split_dict_path = '/workspace/data/data_folder/o2o/split_dict.pkl'\n",
    "combined_features_file = '/workspace/data/data_folder/o2o/all_features/gifs_11/all_features_11.pkl'\n",
    "\n",
    "train, test, val = split(split_dict_path, combined_features_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2055/2055 [09:08<00:00,  3.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "test_dir = '/workspace/data/data_folder/o2o/gifs_11_features_vsgnet'\n",
    "from glob import glob as glob\n",
    "\n",
    "list_files = glob(test_dir+ '/*.pt')\n",
    "issue_files = []\n",
    "\n",
    "from tqdm import tqdm as tqdm\n",
    "for f in tqdm(list_files):\n",
    "    \n",
    "    data = torch.load(f)\n",
    "    if not torch.is_tensor(data['frame_deep_features']):\n",
    "        issue_files.append(f)\n",
    "    del data\n",
    "print(len(issue_files))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/data/data_folder/o2o/gifs_11_features_vsgnet/0dqx7VOjiJI_2426_5.pt',\n",
       " '/workspace/data/data_folder/o2o/gifs_11_features_vsgnet/0dqx7VOjiJI_1572_5.pt',\n",
       " '/workspace/data/data_folder/o2o/gifs_11_features_vsgnet/0dqx7VOjiJI_2358_5.pt',\n",
       " '/workspace/data/data_folder/o2o/gifs_11_features_vsgnet/0dqx7VOjiJI_2357_5.pt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "o2o_dir = '/workspace/data/data_folder/o2o/gifs_11_features_vsgnet'\n",
    "from glob import glob as glob\n",
    "\n",
    "o2o_files = glob(o2o_dir + '/*.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "issues = []\n",
    "\n",
    "new_keys = ['metadata', 'num_obj', 'bboxes', 'lr', 'mr', 'cr', 'object_pairs', 'num_relation', 'frame_deep_features']\n",
    "\n",
    "for f in o2o_files:\n",
    "    \n",
    "    data = torch.load(f)\n",
    "    new_data = {}\n",
    "    \n",
    "    for n in new_keys:\n",
    "        new_data[n] = data[n]\n",
    "    torch.save(new_data, f)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
