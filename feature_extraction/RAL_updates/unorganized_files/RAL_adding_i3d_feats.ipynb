{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotation file\n",
    "anno_path = '/workspace/work/misc/O2ONet/data/annotations_minus_unavailable_yt_vids.pkl'\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "f = open(anno_path, 'rb')\n",
    "anno = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For i3d based bbox features\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "import pytorchvideo.models as models\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self, submodule, layer):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.pretrain_model = submodule\n",
    "        self.layer = layer\n",
    "        \n",
    "        self.layer_list = list(self.pretrain_model._modules['blocks']._modules.keys())\n",
    "        output_layer = self.layer_list[self.layer]  # just change the number of the layer to get the output\n",
    "\n",
    "        self.children_list = []\n",
    "        for (name, comp_layer) in self.pretrain_model._modules['blocks'].named_children():\n",
    "            self.children_list.append(comp_layer)\n",
    "            if name == output_layer:\n",
    "                break\n",
    "        self.feature_extrac_net = nn.Sequential(*self.children_list)\n",
    "        self.pretrain_model = None\n",
    "\n",
    "    def forward(self, image):\n",
    "        feature = self.feature_extrac_net(image)\n",
    "        return feature\n",
    "\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import  NormalizeVideo\n",
    "from pytorchvideo import transforms\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def read_gif(gif_path):\n",
    "    \"\"\"read gif and return dictionary as key ['video'] and value the tensor of size(CxTxHXW)\"\"\"\n",
    "\n",
    "    video = EncodedVideo.from_path(gif_path)\n",
    "    video = video.get_clip(0, 5) # get_clip fetches the clip from starting time to ending time\n",
    "\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master Feature Generator for VSGNet\n",
    "import torch\n",
    "\n",
    "'''\n",
    "get the current dictionary - DONE\n",
    "get the current gif_folder - DONE\n",
    "get the i3d feature extractor\n",
    "and the transform\n",
    "and the device\n",
    "'''\n",
    "def master_feature_generator(annotations, current_file, \n",
    "                             gif_folder, \n",
    "                             i3d_feature_extractor,\n",
    "                             i3d_transform, device):\n",
    "\n",
    "    current_dict = torch.load(current_file)\n",
    "\n",
    "    # Getting details to load the GIF\n",
    "    yt_id = current_dict['metadata']['yt_id']\n",
    "    frame_index = current_dict['metadata']['frame no.']\n",
    "\n",
    "    # locate the annotation\n",
    "    current_anno = None\n",
    "    for a in annotations:\n",
    "        temp_yt_id = a['metadata']['yt_id']\n",
    "        temp_frame_no = a['metadata']['frame no.']\n",
    "        if temp_yt_id == yt_id and temp_frame_no == frame_index:\n",
    "            current_anno = a\n",
    "            break\n",
    "    if current_anno == None:\n",
    "        raise ValueError('Annotation Element not found')\n",
    "\n",
    "    # 'bboxes': {'0': {'class': 'generic_object', 'bbox': [35, 176, 1277, 343]},\n",
    "    # '1': {'class': 'hand', 'bbox': [1113, 116, 1233, 181]},\n",
    "    # '2': {'class': 'hand', 'bbox': [663, 81, 843, 176]}}        \n",
    "\n",
    "    bbox_data = current_anno['bboxes']\n",
    "    keys  = list(bbox_data.keys())\n",
    "    classes = [-1 for i_ in range(12)]\n",
    "    \n",
    "    for i, k in enumerate(keys):\n",
    "        temp_class = bbox_data[k]['class']\n",
    "\n",
    "        if temp_class == 'generic_object':\n",
    "            classes[i] = 0\n",
    "            continue\n",
    "\n",
    "        if temp_class == 'hand':\n",
    "            classes[i] = 1\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('object class not recognized ', + str(temp_class))\n",
    "    \n",
    "    current_dict['obj_classes'] = classes\n",
    "    \n",
    "    window_size = 5\n",
    "\n",
    "    # Loading the gif    \n",
    "    filename = yt_id + '_' + str(frame_index) + '_' + str(window_size) + '.gif'\n",
    "    import os\n",
    "    file_location = os.path.join(gif_folder, filename)\n",
    "\n",
    "    # i3d features\n",
    "    temp_i3d_video = read_gif(file_location)\n",
    "    temp_i3d_video = i3d_transform(temp_i3d_video)[\"video\"]\n",
    "    temp_i3d_video = temp_i3d_video.unsqueeze(0).to(device)\n",
    "\n",
    "    temp_i3d_feature_map = i3d_feature_extractor(temp_i3d_video)\n",
    "\n",
    "    current_dict['i3d_fmap'] = temp_i3d_feature_map[0].to(torch.device('cpu'))\n",
    "    current_dict['obj_class_map'] = {'Hand' : 1, 'Generic Object' : 0}\n",
    "        \n",
    "    return current_dict, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the annotation file\n",
    "anno_path = '/workspace/work/misc/O2ONet/data/annotations_minus_unavailable_yt_vids.pkl'\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "f = open(anno_path, 'rb')\n",
    "annotations = pkl.load(f)\n",
    "f.close()\n",
    "\n",
    "gif_folder = '/workspace/data/data_folder/o2o/gifs_11'\n",
    "\n",
    "import torchvision\n",
    "device = torch.device('cuda:3') if torch.cuda.is_available() else torch.device('cpu')\n",
    "layer_no = 5\n",
    "\n",
    "# i3d feature extractor\n",
    "import pytorchvideo.models as models\n",
    "model_name = \"i3d_r50\"\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo:main\", model=model_name, pretrained=True)\n",
    "model = model.to(device)\n",
    "i3d_feature_net = FeatureExtractor(model, layer_no)\n",
    "\n",
    "# i3d transform\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "i3d_transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(11),\n",
    "            Resize((720,1280)),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std)    \n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2052/2052 [1:34:10<00:00,  2.75s/it]  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "gif_folder = '/workspace/data/data_folder/o2o/gifs_11'\n",
    "from glob import glob as glob\n",
    "file_list = glob('/workspace/data/data_folder/o2o/gifs_11_features_vsgnet/*.pt')\n",
    "# from tqdm import tqdm as tqdm\n",
    "saving_folder = '/workspace/data/data_folder/o2o/gifs_11_features_ral'\n",
    "\n",
    "errors = {}\n",
    "errors['file'] = []\n",
    "errors['exceptions'] = []\n",
    "\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "for f in tqdm(file_list):\n",
    "\n",
    "    try:    \n",
    "        file_name = f.split('/')[-1]\n",
    "        file_location = os.path.join(saving_folder, file_name)\n",
    "        res, success = master_feature_generator(annotations, f, gif_folder, i3d_feature_net, \n",
    "                                                i3d_transform, device)\n",
    "        torch.save(res, file_location)\n",
    "    except Exception as e:\n",
    "        print(\"Issue\")\n",
    "        errors['file'] = file_name\n",
    "        errors['exceptions'] = e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': [], 'exceptions': []}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['metadata', 'num_obj', 'bboxes', 'lr', 'mr', 'cr', 'object_pairs', 'num_relation', 'frame_deep_features', 'i3d_fmap', 'obj_classes', 'obj_class_map'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
