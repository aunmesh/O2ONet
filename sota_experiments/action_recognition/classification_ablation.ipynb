{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "class MLPClassifier_pytorch:\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.model = self.MLP(self.input_dim)\n",
    "\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    class MLP(torch.nn.Module):\n",
    "        \n",
    "        def __init__(self, input_dim, output_dim=11):\n",
    "            super().__init__()\n",
    "\n",
    "            self.input_fc = torch.nn.Linear(input_dim, 1024)\n",
    "            self.output_fc = torch.nn.Linear(1024, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            # x = [batch size, height, width]\n",
    "\n",
    "            batch_size = x.shape[0]\n",
    "            # feat_dim = x.shape[-1]\n",
    "            \n",
    "            x = x.view(batch_size,-1)\n",
    "\n",
    "            # x = [batch size, height * width]\n",
    "\n",
    "            h_1 = torch.nn.functional.relu(self.input_fc(x))\n",
    "\n",
    "            y_pred = self.output_fc(h_1)\n",
    "\n",
    "            y_pred = torch.nn.functional.softmax(y_pred, dim=1)\n",
    "            # y_pred = [batch size, output dim]\n",
    "\n",
    "            return y_pred\n",
    "    \n",
    "    def calculate_accuracy(self, y_pred, y):\n",
    "\n",
    "        top_pred = y_pred.argmax(1, keepdim=True)\n",
    "        correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "        acc = correct.float() / y.shape[0]\n",
    "\n",
    "        return acc.numpy()\n",
    "\n",
    "\n",
    "    def train(self, x, y, model, optimizer, criterion, device):\n",
    "\n",
    "\n",
    "        x_ = torch.from_numpy(x).to(device).float()\n",
    "        import torch.nn.functional as f\n",
    "        y_ = torch.from_numpy(y).type(torch.LongTensor).to(device).float()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x_)\n",
    "       \n",
    "        y_ = y_.type(torch.LongTensor)\n",
    "        \n",
    "        # y_special = F.one_hot(y_, num_classes=11)\n",
    "        loss = criterion(y_pred, y_)\n",
    "\n",
    "        acc = self.calculate_accuracy(y_pred, y_)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "        self.model = self.model.to(device)\n",
    "        \n",
    "        num_epochs = 10\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "\n",
    "        for n in range(num_epochs):\n",
    "            self.train(X_train, Y_train, self.model, optimizer, criterion, device)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, x):\n",
    "        device = torch.device('cpu')\n",
    "        x_ = torch.from_numpy(x).to(device).float()\n",
    "        y_ = self.model(x_)\n",
    "        y_ret = y_.detach().cpu().numpy()\n",
    "        \n",
    "        y_ret = np.argmax(y_ret, axis=1)\n",
    "        \n",
    "        return y_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_everything(selection, ooi_data):\n",
    "\n",
    "    # selection = ['scr', 'mr']\n",
    "    # selection = ['lr']\n",
    "    # selection = ['scr']\n",
    "    import pickle as pkl\n",
    "\n",
    "    import io\n",
    "    import torch\n",
    "\n",
    "    all_labels = []\n",
    "\n",
    "    for d in ooi_data:\n",
    "        temp_labels = []\n",
    "        \n",
    "        for r in d['relations']:\n",
    "            rels = r[1]\n",
    "            \n",
    "            # for k in selection:\n",
    "            #     temp_labels+=rels[k]\n",
    "            \n",
    "            temp_labels+=rels['scr']\n",
    "            temp_labels+=rels['lr']\n",
    "            temp_labels+=rels['mr']\n",
    "            \n",
    "        all_labels+= temp_labels\n",
    "            \n",
    "    unique_labels = list(set(all_labels))\n",
    "    labels_enumeration = {}\n",
    "\n",
    "    for i, u in enumerate(unique_labels):\n",
    "        \n",
    "        if u == '':\n",
    "            continue\n",
    "        labels_enumeration[u] = i-1\n",
    "\n",
    "    # print(labels_enumeration)\n",
    "    all_videos = []\n",
    "    vid_to_class = {}\n",
    "\n",
    "    for d in ooi_data:\n",
    "        all_videos.append( d['metadata']['yt_id'] )\n",
    "        vid_to_class[ d['metadata']['yt_id'] ] = d['metadata']['activity name']\n",
    "\n",
    "    unique_videos = list(set(all_videos))\n",
    "    videos_enumeration = {}\n",
    "\n",
    "    for i, u in enumerate(unique_videos):\n",
    "        \n",
    "        videos_enumeration[u] = i\n",
    "\n",
    "    # print(videos_enumeration)\n",
    "    # print(vid_to_class)\n",
    "    activity_enumeration = {'AssembleCabinet': 0, 'ChangeCarTire': 1, 'FuelCar': 2, 'InstallBicycleRack': 3,\n",
    "    'InstallShowerHead': 4, 'ParkParallel': 5, 'ReplaceDoorKnob': 6, 'ReplaceToiletSeat': 7,\n",
    "    'UseJack': 8, 'PolishCar': 9, 'ReplaceBatteryOnTVControl': 10}\n",
    "    import numpy as np\n",
    "\n",
    "    def make_hoi_features(ooi_data, videos_enumeration, labels_enumeration, activity_enumeration,  vid_to_class_map\n",
    "                        , hoi_ooi_both='hoi'):\n",
    "\n",
    "        num_rows = len( list(videos_enumeration.keys()) )\n",
    "        num_cols = len( list(labels_enumeration.keys()) )\n",
    "        \n",
    "        X = np.zeros((num_rows, num_cols))\n",
    "        Y = np.zeros(num_rows)\n",
    "        num_gifs_for_each_vid = np.zeros(num_rows)\n",
    "        \n",
    "        num_gifs = len(ooi_data)\n",
    "        \n",
    "        for i in range(num_gifs):\n",
    "\n",
    "            curr_vid = ooi_data[i]['metadata']['yt_id']\n",
    "            curr_activity = vid_to_class_map[curr_vid]\n",
    "            curr_activity_enumeration = activity_enumeration[curr_activity]\n",
    "\n",
    "            curr_row = videos_enumeration[curr_vid]\n",
    "            \n",
    "            \n",
    "            Y[curr_row] = curr_activity_enumeration\n",
    "            \n",
    "            \n",
    "            objs = ooi_data[i]['bboxes']\n",
    "            \n",
    "            human_boxes = []\n",
    "            obj_keys = list(objs.keys())\n",
    "            \n",
    "            for o in obj_keys:\n",
    "                temp_cat = objs[o]['class']\n",
    "                if temp_cat == 'hand':\n",
    "                    human_boxes.append(int(o))\n",
    "            \n",
    "            if len(human_boxes) == 0:\n",
    "                continue\n",
    "            \n",
    "            temp_rels = ooi_data[i]['relations']\n",
    "\n",
    "            for r in temp_rels:\n",
    "\n",
    "                obj1, obj2 = r[0]\n",
    "                \n",
    "                if hoi_ooi_both == 'hoi':\n",
    "                    if not((obj1 in human_boxes) or (obj2 in human_boxes)):\n",
    "                        continue\n",
    "\n",
    "                if hoi_ooi_both == 'ooi':\n",
    "                    if (obj1 in human_boxes) or (obj2 in human_boxes):\n",
    "                        continue\n",
    "\n",
    "                \n",
    "                temp_labels = []\n",
    "\n",
    "                rels = r[1]\n",
    "                \n",
    "                for k in selection:\n",
    "                    temp_labels+=rels[k]\n",
    "\n",
    "                for t in temp_labels:\n",
    "                    if t == '':\n",
    "                        continue\n",
    "                    temp_col = labels_enumeration[t]\n",
    "                    X[curr_row, temp_col]+=1\n",
    "            \n",
    "            num_gifs_for_each_vid[curr_row] += 1\n",
    "        \n",
    "        return [X, Y, num_gifs_for_each_vid]\n",
    "\n",
    "    from copy import deepcopy as copy\n",
    "\n",
    "    def normalize(X, Y, count, softmax=1):\n",
    "        \n",
    "        offset = 0.00001\n",
    "        \n",
    "        X_ret = copy(X)\n",
    "        \n",
    "        normalizing_vec = np.sum(X, axis=1) + offset\n",
    "        normalizing_vec = np.expand_dims(normalizing_vec, 1)\n",
    "        \n",
    "        # print(normalizing_vec.shape, X_ret.shape)\n",
    "        \n",
    "        X_ret = np.divide(X_ret, normalizing_vec)\n",
    "\n",
    "        normalizing_vec = count + offset\n",
    "        normalizing_vec = np.expand_dims(normalizing_vec, 1)\n",
    "\n",
    "        X_ret = np.divide(X_ret, normalizing_vec)\n",
    "        \n",
    "        if softmax == 1:\n",
    "            import torch\n",
    "            X_ret = torch.from_numpy(X_ret)\n",
    "            X_ret_ = torch.nn.functional.softmax(X_ret, dim=1)\n",
    "            X_ret = X_ret_.numpy()\n",
    "\n",
    "        return X_ret\n",
    "    X_hoi, Y, num_gifs_for_each_vid = make_hoi_features(ooi_data,\n",
    "                                                        videos_enumeration,\n",
    "                                                        labels_enumeration,\n",
    "                                                        activity_enumeration,\n",
    "                                                        vid_to_class, 'hoi')\n",
    "\n",
    "    X_hoi_normalized = normalize(X_hoi, Y, num_gifs_for_each_vid)\n",
    "\n",
    "    X_ooi, Y, num_gifs_for_each_vid = make_hoi_features(ooi_data,\n",
    "                                                        videos_enumeration,\n",
    "                                                        labels_enumeration,\n",
    "                                                        activity_enumeration,\n",
    "                                                        vid_to_class, 'ooi')\n",
    "\n",
    "    X_ooi_normalized = normalize(X_ooi, Y, num_gifs_for_each_vid)\n",
    "\n",
    "    X_both, Y, num_gifs_for_each_vid = make_hoi_features(ooi_data,\n",
    "                                                        videos_enumeration,\n",
    "                                                        labels_enumeration,\n",
    "                                                        activity_enumeration,\n",
    "                                                        vid_to_class, 'both')\n",
    "\n",
    "    X_both_normalized = normalize(X_both, Y, num_gifs_for_each_vid)\n",
    "    \n",
    "\n",
    "\n",
    "    # print(\" Concatenation is being DONE\")\n",
    "    X_both2 = np.concatenate((X_hoi, X_ooi), axis=1)\n",
    "    \n",
    "    X_both_normalized2 = normalize(X_both2, Y, num_gifs_for_each_vid)\n",
    "    # X_both_normalized2 = X_both2\n",
    "\n",
    "    from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "    from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "    from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    def train_test_individual(X_train, Y_train, X_test, Y_test):\n",
    "\n",
    "        clf = DecisionTreeClassifier()\n",
    "        # clf = KNeighborsClassifier()\n",
    "        input_dim = X_train.shape[1]\n",
    "        # clf = MLPClassifier_pytorch(input_dim=input_dim)\n",
    "        # clf = KNeighborsClassifier()\n",
    "        # clf = GaussianNB()\n",
    "\n",
    "        clf = clf.fit(X_train,Y_train)\n",
    "        \n",
    "        Y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Model Accuracy, how often is the classifier correct?\n",
    "        \n",
    "        acc = metrics.accuracy_score(Y_test, Y_pred)\n",
    "        \n",
    "        del clf\n",
    "        del Y_pred\n",
    "        \n",
    "        return acc\n",
    "\n",
    "    def train_test(X_hoi, X_ooi, X_both, X_both2, Y):\n",
    "        \n",
    "        import random\n",
    "        ind = [i for i in range(438)]\n",
    "        random.shuffle(ind)\n",
    "        ind = np.array(ind)\n",
    "\n",
    "        offset = 350\n",
    "        ind_test = ind[offset:]\n",
    "        ind_train = ind[:offset]\n",
    "\n",
    "        X_hoi_train = X_hoi[ind_train, :]\n",
    "        X_hoi_test = X_hoi[ind_test, :]\n",
    "\n",
    "        X_ooi_train = X_ooi[ind_train, :]\n",
    "        X_ooi_test = X_ooi[ind_test, :]\n",
    "\n",
    "        X_both_train = X_both[ind_train, :]\n",
    "        X_both_test = X_both[ind_test, :]\n",
    "\n",
    "        X_both2_train = X_both2[ind_train, :]\n",
    "        X_both2_test = X_both2[ind_test, :]\n",
    "\n",
    "        Y_train = Y[ind_train]\n",
    "        Y_test = Y[ind_test]\n",
    "\n",
    "        acc_both = train_test_individual(X_both_train, Y_train, X_both_test, Y_test)\n",
    "        acc_both2 = train_test_individual(X_both2_train, Y_train, X_both2_test, Y_test)\n",
    "        acc_ooi = train_test_individual(X_ooi_train, Y_train, X_ooi_test, Y_test)\n",
    "        \n",
    "        acc_hoi = train_test_individual(X_hoi_train, Y_train, X_hoi_test, Y_test)\n",
    "        \n",
    "        Y_random = np.random.randint(0, 12, len(Y_test))\n",
    "        acc_random = metrics.accuracy_score(Y_test, Y_random)\n",
    "\n",
    "        # print(\"Accuracy hoi:\", acc_hoi)\n",
    "        # print(\"Accuracy ooi:\",acc_ooi)\n",
    "        # print(\"Accuracy both:\",acc_both)\n",
    "        # print(\"Accuracy random:\",acc_random)\n",
    "        \n",
    "        acc = [acc_hoi, acc_ooi, acc_both, acc_both2, acc_random]\n",
    "        \n",
    "        return acc\n",
    "\n",
    "    all_accs = []\n",
    "\n",
    "    from tqdm import tqdm as tqdm\n",
    "    for i in tqdm(range(50)):\n",
    "        temp_acc = train_test(X_hoi_normalized, X_ooi_normalized, X_both_normalized, X_both_normalized2, Y)\n",
    "        # temp_acc = train_test(X_hoi, X_ooi, X_both, Y)\n",
    "        all_accs.append(temp_acc)\n",
    "        \n",
    "    all_accs = np.array(all_accs)\n",
    "    all_accs = np.mean(all_accs, axis = 0)\n",
    "    print(selection)\n",
    "    print('hoi ooi both both2 random')\n",
    "    # print(all_accs)\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    features['hoi'] = X_hoi_normalized\n",
    "    features['ooi'] = X_ooi_normalized\n",
    "    features['hoi+ooi'] = X_both_normalized\n",
    "    features['hoi|ooi'] = X_both_normalized2\n",
    "    features['y'] = Y\n",
    "    \n",
    "    return all_accs, features\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import pickle as pkl\n",
    "\n",
    "class CPU_Unpickler(pkl.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else:\n",
    "            return super().find_class(module, name)\n",
    "\n",
    "#contents = pickle.load(f) becomes...\n",
    "f = open('./labels.pkl', 'rb')\n",
    "ooi_data = CPU_Unpickler(f).load()\n",
    "f.close()\n",
    "\n",
    "selection = ['scr', 'lr', 'mr']\n",
    "\n",
    "import itertools\n",
    "selections = []\n",
    "\n",
    "stuff = selection\n",
    "for L in range(len(stuff) + 1):\n",
    "    for subset in itertools.combinations(stuff, L):\n",
    "        if len(subset) > 0:\n",
    "            selections.append(list(subset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 45.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scr']\n",
      "hoi ooi both both2 random\n",
      "[0.22727273 0.18409091 0.19227273 0.24159091 0.08318182]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 41.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lr']\n",
      "hoi ooi both both2 random\n",
      "[0.20863636 0.3        0.27522727 0.32       0.08272727]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 35.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr']\n",
      "hoi ooi both both2 random\n",
      "[0.25409091 0.35022727 0.31068182 0.33954545 0.08454545]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 41.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scr', 'lr']\n",
      "hoi ooi both both2 random\n",
      "[0.27       0.32545455 0.31431818 0.34545455 0.08363636]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 34.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scr', 'mr']\n",
      "hoi ooi both both2 random\n",
      "[0.25704545 0.33113636 0.31       0.35840909 0.08454545]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 36.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lr', 'mr']\n",
      "hoi ooi both both2 random\n",
      "[0.30159091 0.40795455 0.37863636 0.43159091 0.08159091]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 37.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scr', 'lr', 'mr']\n",
      "hoi ooi both both2 random\n",
      "[0.30522727 0.39840909 0.38659091 0.42454545 0.0825    ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# do_everything(selection, ooi_data)\n",
    "\n",
    "for s in selections:\n",
    "    accs, feats = do_everything(s, ooi_data)\n",
    "    print(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 66.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scr']\n",
      "hoi ooi both both2 random\n",
      "[0.23636364 0.18590909 0.18636364 0.25818182 0.08318182]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 54.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lr']\n",
      "hoi ooi both both2 random\n",
      "[0.20477273 0.29795455 0.26636364 0.29613636 0.08022727]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 47.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr']\n",
      "hoi ooi both both2 random\n",
      "[0.24113636 0.33068182 0.30090909 0.32545455 0.07931818]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 56.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scr', 'lr']\n",
      "hoi ooi both both2 random\n",
      "[0.25431818 0.32704545 0.29772727 0.32272727 0.08090909]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 46.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scr', 'mr']\n",
      "hoi ooi both both2 random\n",
      "[0.25545455 0.34204545 0.32068182 0.33772727 0.0875    ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 46.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lr', 'mr']\n",
      "hoi ooi both both2 random\n",
      "[0.275      0.3875     0.37363636 0.38681818 0.07431818]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 46.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scr', 'lr', 'mr']\n",
      "hoi ooi both both2 random\n",
      "[0.30681818 0.41136364 0.36659091 0.39727273 0.08340909]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for s in selections:\n",
    "    accs, feats = do_everything(s, ooi_data)\n",
    "    print(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hoi', 'ooi', 'hoi+ooi', 'hoi|ooi', 'y'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(438, 40)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats['hoi|ooi'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
